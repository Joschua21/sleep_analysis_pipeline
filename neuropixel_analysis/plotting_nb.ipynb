{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c7e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports: \n",
    "from pinkrigs_tools.dataset.query import load_data, queryCSV\n",
    "from pinkrigs_tools.utils.spk_utils import bombcell_sort_units\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "from datetime import datetime\n",
    "import time\n",
    "import seaborn as sns\n",
    "from scipy import stats, signal\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from neuropixel_utils import *\n",
    "from correlation_utils import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Experiment parameters - change to the current experiment:\n",
    "SUBJECT = 'AV049'\n",
    "EXP_DATE = '2023-07-27'\n",
    "EXP_NUM = '2'\n",
    "\n",
    "exp_kwargs = {\n",
    "    'subject': [SUBJECT],\n",
    "    'expDate': EXP_DATE,\n",
    "    'expNum': EXP_NUM,\n",
    "}\n",
    "\n",
    "# Defined DLC folder path and output directory: \n",
    "dlc_folder = f\"C:\\\\Users\\\\Experiment\\\\Projects\\\\video_conversions\\\\subjects\\\\{SUBJECT}\\\\{EXP_DATE}_{EXP_NUM}\\\\DLC\"\n",
    "output_folder = os.path.join(dlc_folder, \"neuropixel_analysis\")\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Define ONE data to load: \n",
    "data_name_dict = {'events':{'_av_trials':'table'}}\n",
    "recordings = load_data(data_name_dict=data_name_dict,**exp_kwargs)\n",
    "exp_info = queryCSV(**exp_kwargs)\n",
    "print(f\"Analysis for: {SUBJECT} on {EXP_DATE} experiment {EXP_NUM}\")\n",
    "print(f\"Experiment folder:\", exp_info['expFolder'].iloc[0], \n",
    "      \"Ephys folder:\", exp_info['ephysPathProbe0'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb973f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load variables from saved results:\n",
    "loaded_results = load_analysis_results(output_folder=output_folder)\n",
    "\n",
    "results = loaded_results['results']\n",
    "freq_results = loaded_results['freq_results']\n",
    "spectrum_results = loaded_results['spectrum_results']\n",
    "smoothed_results = loaded_results['smoothed_results']\n",
    "neural_sleep_csv = loaded_results['neural_sleep_csv']\n",
    "np_results = loaded_results['np_results']\n",
    "pca_results = loaded_results['pca_results']\n",
    "matrix_results = loaded_results['matrix_results']\n",
    "state_corr_results = loaded_results['state_corr_results']\n",
    "rrf_results = loaded_results['rrf_results']\n",
    "population_corr_results = loaded_results['population_corr_results']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5575860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_visualization(results, freq_results, np_results, spectrum_results, \n",
    "                          dlc_folder, output_dir=None, save_plots=False, smoothed_results=None, pca_results=None, show_sleep_in_delta=False):\n",
    "    \"\"\"\n",
    "    Create a combined visualization of sleep-wake activity, power spectrum analysis,\n",
    "    and behavioral data using pre-computed results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Original dictionary containing results from process_spike_data()\n",
    "    freq_results : dict\n",
    "        Original high temporal resolution data for frequency analysis\n",
    "    np_results : dict\n",
    "        Results from analyze_sleep_wake_activity()\n",
    "    spectrum_results : dict\n",
    "        Results from analyze_power_spectrum()\n",
    "    dlc_folder : str\n",
    "        Path to the DLC folder containing behavioral data\n",
    "    output_dir : str, optional\n",
    "        Directory to save plots to (default: None)\n",
    "    save_plots : bool, optional\n",
    "        Whether to save plots (default: False)\n",
    "    show_sleep_in_delta : bool, optional\n",
    "        Whether to show sleep periods in the delta power plot (default: False)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check which probes have the required data\n",
    "    valid_probes = []\n",
    "    for probe in results:\n",
    "        if 'sleep_bout_mapping' not in results[probe]:\n",
    "            print(f\"No sleep bout mapping found for {probe}. Skipping.\")\n",
    "            continue\n",
    "        if 'cluster_quality' not in results[probe]:\n",
    "            print(f\"No quality labels found for {probe}. Skipping.\")\n",
    "            continue\n",
    "        valid_probes.append(probe)\n",
    "    \n",
    "    if not valid_probes:\n",
    "        print(\"No valid probes found with required data.\")\n",
    "        return\n",
    "    \n",
    "    # Use the first probe's time bins and sleep bout info as reference\n",
    "    reference_probe = valid_probes[0]\n",
    "    time_bins = results[reference_probe]['time_bins']\n",
    "    sleep_bouts = results[reference_probe]['sleep_bout_mapping']\n",
    "    \n",
    "    # Create a mask for time bins that fall within any sleep period\n",
    "    sleep_mask = np.zeros(len(time_bins), dtype=bool)\n",
    "    for _, bout in sleep_bouts.iterrows():\n",
    "        start_idx = bout['start_bin_index']\n",
    "        end_idx = bout['end_bin_index']\n",
    "        sleep_mask[start_idx:end_idx+1] = True\n",
    "\n",
    "    # Time extent for all plots\n",
    "    time_extent = [time_bins[0], time_bins[-1]]\n",
    "    \n",
    "    # Load behavioral data from DLC pixel_difference folder\n",
    "    pixel_diff_path = os.path.join(dlc_folder, \"pixel_difference\")\n",
    "    \n",
    "    # Find the CSV file containing pixel differences\n",
    "    pixel_diff_files = [f for f in os.listdir(pixel_diff_path) \n",
    "                      if f.endswith('.csv') and 'pixel_differences' in f]\n",
    "    \n",
    "    if not pixel_diff_files:\n",
    "        print(f\"No pixel difference CSV found in {pixel_diff_path}\")\n",
    "        behavior_data = None\n",
    "    else:\n",
    "        # Use the first matching file\n",
    "        pixel_diff_file = os.path.join(pixel_diff_path, pixel_diff_files[0])\n",
    "        try:\n",
    "            behavior_data = pd.read_csv(pixel_diff_file)\n",
    "            print(f\"Loaded behavioral data from {pixel_diff_file}\")\n",
    "            \n",
    "            # Check if the required column exists\n",
    "            if 'smoothed_difference' not in behavior_data.columns or 'time_sec' not in behavior_data.columns:\n",
    "                print(f\"Required columns not found in {pixel_diff_file}\")\n",
    "                print(f\"Available columns: {list(behavior_data.columns)}\")\n",
    "                behavior_data = None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading behavioral data: {e}\")\n",
    "            behavior_data = None\n",
    "    \n",
    "    # Create figure with 5 subplots with custom height ratios (removed PC1 plot)\n",
    "    fig = plt.figure(figsize=(16, 20))\n",
    "    gs = gridspec.GridSpec(5, 1, height_ratios=[1, 3, 1, 3, 1.5], figure=fig)\n",
    "    \n",
    "    # ================ PLOT 1: BEHAVIOR DATA (NO SLEEP PERIODS) ================\n",
    "    ax_behav = fig.add_subplot(gs[0])\n",
    "    \n",
    "    if behavior_data is not None:\n",
    "        # Extract time and smoothed difference data\n",
    "        time_sec = behavior_data['time_sec'].values\n",
    "        smoothed_diff = behavior_data['smoothed_difference'].values\n",
    "        \n",
    "        # Convert to thousands for better axis labels\n",
    "        smoothed_diff_scaled = smoothed_diff / 1000\n",
    "        \n",
    "        # Handle missing values - create separate segments to plot\n",
    "        valid_mask = ~np.isnan(smoothed_diff_scaled)\n",
    "        gaps = np.where(np.diff(valid_mask.astype(int)) != 0)[0] + 1\n",
    "        segments = np.split(np.arange(len(valid_mask)), gaps)\n",
    "        \n",
    "        # Plot each continuous segment separately (NO SLEEP PERIODS)\n",
    "        for segment in segments:\n",
    "            if len(segment) > 0 and valid_mask[segment[0]]:  # Only plot valid segments\n",
    "                ax_behav.plot(time_sec[segment], smoothed_diff_scaled[segment], 'k-', linewidth=1)\n",
    "        \n",
    "        ax_behav.set_title('Movement Activity')\n",
    "        ax_behav.set_ylabel('Pixel Difference')\n",
    "        ax_behav.set_xlim(time_extent)\n",
    "        ax_behav.grid(True, alpha=0.2)\n",
    "    else:\n",
    "        ax_behav.text(0.5, 0.5, 'Behavioral data not available', \n",
    "                    ha='center', va='center', transform=ax_behav.transAxes)\n",
    "        ax_behav.set_xlim(time_extent)\n",
    "    \n",
    "    # ================ PLOT 2: CLUSTER ACTIVITY HEATMAP (NO SLEEP INDICATORS) ================\n",
    "    ax1 = fig.add_subplot(gs[1], sharex=ax_behav)\n",
    "    \n",
    "    # Get sorted normalized counts from np_results if available\n",
    "    if 'merged' in np_results and 'modulation_index' in np_results['merged']:\n",
    "        # Extract data from np_results\n",
    "        merged_data = np_results['merged']\n",
    "        modulation_index = merged_data['modulation_index']\n",
    "        \n",
    "        # Sort by modulation index\n",
    "        sorted_indices = np.argsort(modulation_index)\n",
    "        \n",
    "        # Get or reconstruct normalized counts\n",
    "        if 'counts' in merged_data:\n",
    "            sorted_normalized_counts = merged_data['counts'][sorted_indices]\n",
    "        else:\n",
    "            # Need to reconstruct from original data\n",
    "            # Collect data from all probes\n",
    "            all_counts = []\n",
    "            for probe in valid_probes:\n",
    "                # Filter out noise clusters, keep good and mua\n",
    "                counts, _, _ = filter_clusters_by_quality(\n",
    "                    results, probe, include_qualities=['good', 'mua']\n",
    "                )\n",
    "                all_counts.append(counts)\n",
    "                \n",
    "            merged_counts = np.vstack(all_counts)\n",
    "            \n",
    "            # Normalize and sort\n",
    "            normalized_counts = np.zeros_like(merged_counts, dtype=float)\n",
    "            for i in range(merged_counts.shape[0]):\n",
    "                cluster_counts = merged_counts[i, :]\n",
    "                p95 = np.percentile(cluster_counts, 95)\n",
    "                \n",
    "                # Avoid division by zero\n",
    "                if p95 > 0:\n",
    "                    normalized_counts[i, :] = cluster_counts / p95\n",
    "                else:\n",
    "                    normalized_counts[i, :] = cluster_counts\n",
    "                    \n",
    "            sorted_normalized_counts = normalized_counts[sorted_indices]\n",
    "    else:\n",
    "        print(\"Warning: modulation_index not found in np_results. Cannot create sorted heatmap.\")\n",
    "        sorted_normalized_counts = None\n",
    "        \n",
    "    if sorted_normalized_counts is not None:\n",
    "        # Plot the heatmap (NO SLEEP INDICATORS)\n",
    "        extent = [time_extent[0], time_extent[1], 0, sorted_normalized_counts.shape[0]]\n",
    "        vmax_threshold = np.percentile(sorted_normalized_counts, 95)\n",
    "        \n",
    "        im1 = ax1.matshow(sorted_normalized_counts, aspect='auto', extent=extent, cmap='binary', \n",
    "                        interpolation='none', origin='lower', vmin=0, vmax=vmax_threshold)\n",
    "        \n",
    "        ax1.set_ylabel('Clusters')\n",
    "        ax1.set_xlim(time_extent)\n",
    "    \n",
    "    # ================ PLOT 3: AVERAGE ACTIVITY (NO SLEEP PERIODS) ================\n",
    "    ax2 = fig.add_subplot(gs[2], sharex=ax_behav)\n",
    "    \n",
    "    # Get or calculate mean activity\n",
    "    if 'merged' in np_results and 'mean_activity' in np_results['merged']:\n",
    "        mean_activity = np_results['merged']['mean_activity']\n",
    "    else:\n",
    "        # Need to calculate from original data\n",
    "        all_counts = []\n",
    "        for probe in valid_probes:\n",
    "            # Filter out noise clusters, keep good and mua\n",
    "            counts, _, _ = filter_clusters_by_quality(\n",
    "                results, probe, include_qualities=['good', 'mua']\n",
    "            )\n",
    "            all_counts.append(counts)\n",
    "            \n",
    "        if all_counts:\n",
    "            merged_counts = np.vstack(all_counts)\n",
    "            mean_activity = np.mean(merged_counts, axis=0)\n",
    "        else:\n",
    "            mean_activity = None\n",
    "            print(\"Warning: No valid clusters found to calculate mean activity\")\n",
    "    \n",
    "    if mean_activity is not None:\n",
    "        # Calculate reasonable y-limits with padding\n",
    "        buffer_factor = 0.15\n",
    "        data_range = np.max(mean_activity) - np.min(mean_activity)\n",
    "        buffer_amount = data_range * buffer_factor\n",
    "        y_max = np.max(mean_activity) + buffer_amount\n",
    "        y_min = np.min(mean_activity) - buffer_amount\n",
    "        \n",
    "        ax2.plot(time_bins, mean_activity, color='black', linewidth=1)\n",
    "        ax2.set_ylim(y_min, y_max)\n",
    "        \n",
    "        ax2.grid(True, alpha=0.2)\n",
    "        ax2.set_title('Average activity across all filtered clusters')\n",
    "        ax2.set_ylabel('Mean spike count')\n",
    "        ax2.set_xlim(time_extent)\n",
    "    \n",
    "    # ================ PLOT 4: POWER SPECTRUM ================\n",
    "    ax4 = fig.add_subplot(gs[3], sharex=ax_behav)\n",
    "    \n",
    "    # Get or calculate power spectrum\n",
    "    if 'merged' in spectrum_results and 'power_spectrum' in spectrum_results['merged']:\n",
    "        spectrum_data = spectrum_results['merged']\n",
    "        frequencies = spectrum_data['frequencies']\n",
    "        Sxx = spectrum_data['power_spectrum']\n",
    "        \n",
    "        # Convert to dB if not already\n",
    "        if not np.any(Sxx < 0):  # Assuming it's not already in dB\n",
    "            Sxx_db = 10 * np.log10(Sxx + 1e-10)\n",
    "        else:\n",
    "            Sxx_db = Sxx\n",
    "        \n",
    "        # Apply frequency filtering\n",
    "        freq_range = (0, 30)\n",
    "        freq_mask = (frequencies >= freq_range[0]) & (frequencies <= freq_range[1])\n",
    "        frequencies_filtered = frequencies[freq_mask]\n",
    "        Sxx_db_filtered = Sxx_db[freq_mask, :]\n",
    "    else:\n",
    "        print(\"Warning: power_spectrum not found in spectrum_results. Cannot create spectrogram.\")\n",
    "        Sxx_db_filtered = None\n",
    "        frequencies_filtered = None\n",
    "    \n",
    "    if Sxx_db_filtered is not None and frequencies_filtered is not None:\n",
    "        # Calculate power limits\n",
    "        power_5th = np.percentile(Sxx_db_filtered, 5)\n",
    "        power_95th = np.percentile(Sxx_db_filtered, 95)\n",
    "        vmin = power_5th - 5\n",
    "        vmax = power_95th + 5\n",
    "        \n",
    "        # Get frequency time bins\n",
    "        if 'times' in spectrum_data:\n",
    "            spec_times = spectrum_data['times']\n",
    "            # Calculate spectrogram extent\n",
    "            spec_extent = [time_extent[0], time_extent[1], frequencies_filtered[0], frequencies_filtered[-1]]\n",
    "        else:\n",
    "            # Retrieve from freq_results\n",
    "            freq_time_bins = freq_results[reference_probe]['time_bins']\n",
    "            spec_extent = [freq_time_bins[0], freq_time_bins[-1], frequencies_filtered[0], frequencies_filtered[-1]]\n",
    "        \n",
    "        im3 = ax4.matshow(Sxx_db_filtered, aspect='auto', origin='lower', \n",
    "                        extent=spec_extent, cmap='viridis',\n",
    "                        vmin=vmin, vmax=vmax)\n",
    "        \n",
    "        ax4.set_title('Power Spectrum')\n",
    "        ax4.set_ylabel('Frequency (Hz)')\n",
    "        ax4.set_xlim(time_extent)  # Use the same time extent as other plots\n",
    "    \n",
    "    # ================ PLOT 5: FREQUENCY BAND POWER (OPTIONAL SLEEP PERIODS) ================\n",
    "    ax5 = fig.add_subplot(gs[4], sharex=ax_behav)\n",
    "\n",
    "    # Check if we have smoothed data available from the smoothed_results parameter\n",
    "    band_powers = None\n",
    "    smoothed_available = False\n",
    "    filter_type = None\n",
    "    \n",
    "    # First check if smoothed_results is provided directly\n",
    "    if smoothed_results is not None:\n",
    "        # Try to determine which filter was used in save_sleep_periods_to_csv\n",
    "        if output_dir:\n",
    "            sleep_times_csv = os.path.join(output_dir, \"sleep_times.csv\")\n",
    "            if os.path.exists(sleep_times_csv):\n",
    "                try:\n",
    "                    sleep_df = pd.read_csv(sleep_times_csv)\n",
    "                    if 'filter' in sleep_df.columns and len(sleep_df) > 0:\n",
    "                        filter_name = sleep_df['filter'].iloc[0]\n",
    "                        if 'Savitzky-Golay' in filter_name:\n",
    "                            filter_type = 'sg'\n",
    "                            print(\"Using Savitzky-Golay filter for frequency band visualization\")\n",
    "                            if 'savitzky_golay' in smoothed_results:\n",
    "                                band_powers = smoothed_results['savitzky_golay']\n",
    "                                smoothed_available = True\n",
    "                        elif 'MovingAverage' in filter_name or 'Moving Average' in filter_name:\n",
    "                            filter_type = 'ma'\n",
    "                            print(\"Using Moving Average filter for frequency band visualization\")\n",
    "                            if 'moving_average' in smoothed_results:\n",
    "                                band_powers = smoothed_results['moving_average']\n",
    "                                smoothed_available = True\n",
    "                except Exception as e:\n",
    "                    print(f\"Error determining filter type: {e}\")\n",
    "    \n",
    "    # Fall back to original if smoothed not available\n",
    "    if not smoothed_available and 'merged' in spectrum_results and 'band_powers' in spectrum_results['merged']:\n",
    "        band_powers = spectrum_results['merged']['band_powers']\n",
    "        print(\"Using original band powers (smoothed data not available)\")\n",
    "    \n",
    "    if band_powers is not None:\n",
    "        # Get or calculate spectrogram times\n",
    "        if 'times' in spectrum_results.get('merged', {}):\n",
    "            spec_times = np.linspace(time_extent[0], time_extent[1], len(next(iter(band_powers.values()))))\n",
    "        else:\n",
    "            # Retrieve from freq_results\n",
    "            freq_time_bins = freq_results[reference_probe]['time_bins']\n",
    "            spec_times = np.linspace(freq_time_bins[0], freq_time_bins[-1], len(next(iter(band_powers.values()))))\n",
    "        \n",
    "        # Plot each band\n",
    "        for band_name, power in band_powers.items():\n",
    "            ax5.plot(spec_times, power, label=f\"{band_name} {'(Smoothed)' if smoothed_available else ''}\", linewidth=1.5)\n",
    "        \n",
    "        # Optionally add sleep periods as shaded regions\n",
    "        if show_sleep_in_delta:\n",
    "            y_limits = ax5.get_ylim()\n",
    "            for _, bout in sleep_bouts.iterrows():\n",
    "                ax5.axvspan(bout['start_timestamp_s'], bout['end_timestamp_s'], \n",
    "                           alpha=0.2, color='lightblue')\n",
    "            ax5.set_ylim(y_limits)  # Restore original y-limits\n",
    "        \n",
    "        ax5.set_title('Delta Power over Time (smoothed)')\n",
    "        ax5.set_ylabel('Power (dB)')\n",
    "        ax5.set_xlabel('Time (s)')\n",
    "        ax5.legend(loc='upper right')\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "        ax5.set_xlim(time_extent)  # Use the same time extent as other plots\n",
    "    \n",
    "    # Adjust layout to make better use of space now that colorbars are removed\n",
    "    plt.subplots_adjust(hspace=0.4, left=0.06, right=0.98, top=0.96, bottom=0.05)\n",
    "    \n",
    "    # Save plot if requested\n",
    "    if save_plots and output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"combined_sleep_wake_spectrum_analysis_{timestamp}.png\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved combined plot to: {filepath}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb321d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_visualization(\n",
    "    results=results,\n",
    "    freq_results=freq_results,\n",
    "    np_results=np_results,\n",
    "    spectrum_results=spectrum_results,\n",
    "    dlc_folder=dlc_folder,\n",
    "    output_dir=output_folder,\n",
    "    save_plots=True,\n",
    "    smoothed_results=smoothed_results,\n",
    "    pca_results=pca_results,\n",
    "    show_sleep_in_delta=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ea2711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cluster_state_and_stability(results, output_dir=None, save_plots=False, bin_size_s=120, \n",
    "                                       state_threshold=0.9, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Combined analysis of cluster state distribution and neuronal stability using pooled data from all probes.\n",
    "    Creates both the state distribution plot and stability scatter plots in a single analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Dictionary containing results from process_spike_data(), with sleep bout mapping and bombcell classification\n",
    "    output_dir : str, optional\n",
    "        Directory to save plots to (default: None)\n",
    "    save_plots : bool, optional\n",
    "        Whether to save plots (default: False)\n",
    "    bin_size_s : float, optional\n",
    "        Size of time bins in seconds for state analysis (default: 120s = 2 minutes)\n",
    "    state_threshold : float, optional\n",
    "        Threshold for classifying a bin as sleep/wake (default: 0.9 or 90%)\n",
    "    max_iterations : int, optional\n",
    "        Maximum number of iterations for random assignment (default: 1000)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing both state distribution and stability metrics for merged probes\n",
    "    \"\"\"\n",
    "    # Convert threshold from percentage to fraction if needed\n",
    "    if state_threshold > 1:\n",
    "        state_threshold = state_threshold / 100\n",
    "    \n",
    "    # Check which probes have the required data\n",
    "    valid_probes = []\n",
    "    for probe in results:\n",
    "        if 'sleep_bout_mapping' not in results[probe]:\n",
    "            print(f\"No sleep bout mapping found for {probe}. Skipping.\")\n",
    "            continue\n",
    "        if 'cluster_quality' not in results[probe]:\n",
    "            print(f\"No quality labels found for {probe}. Skipping.\")\n",
    "            continue\n",
    "        valid_probes.append(probe)\n",
    "    \n",
    "    if not valid_probes:\n",
    "        print(\"No valid probes found with required data.\")\n",
    "        return {}\n",
    "    \n",
    "    # Use the first probe's time bins and sleep bout info as reference\n",
    "    reference_probe = valid_probes[0]\n",
    "    time_bins = results[reference_probe]['time_bins']\n",
    "    sleep_bouts = results[reference_probe]['sleep_bout_mapping']\n",
    "    original_bin_size = time_bins[1] - time_bins[0]\n",
    "    \n",
    "    # Create a mask for time bins that fall within any sleep period\n",
    "    sleep_mask = np.zeros(len(time_bins), dtype=bool)\n",
    "    for _, bout in sleep_bouts.iterrows():\n",
    "        start_idx = bout['start_bin_index']\n",
    "        end_idx = bout['end_bin_index']\n",
    "        sleep_mask[start_idx:end_idx+1] = True\n",
    "\n",
    "    wake_mask = ~sleep_mask\n",
    "    \n",
    "    # Collect and merge data from all probes\n",
    "    all_counts = []\n",
    "    all_cluster_ids = []\n",
    "    all_min_length = float('inf')\n",
    "    \n",
    "    # First pass: determine minimum length and collect data\n",
    "    for probe in valid_probes:\n",
    "        # Filter out noise clusters, keep good and mua\n",
    "        counts, cluster_ids, quality_mask = filter_clusters_by_quality(\n",
    "            results, probe, include_qualities=['good', 'mua']\n",
    "        )\n",
    "        \n",
    "        if counts.shape[0] > 0:\n",
    "            all_min_length = min(all_min_length, counts.shape[1])\n",
    "            all_counts.append(counts)\n",
    "            all_cluster_ids.extend([(probe, cid) for cid in cluster_ids])\n",
    "    \n",
    "    if not all_counts:\n",
    "        print(\"No valid clusters found after filtering.\")\n",
    "        return {}\n",
    "    \n",
    "    # Second pass: truncate all arrays to minimum length\n",
    "    for i in range(len(all_counts)):\n",
    "        all_counts[i] = all_counts[i][:, :all_min_length]\n",
    "    \n",
    "    # Merge counts from all probes\n",
    "    merged_counts = np.vstack(all_counts)\n",
    "    \n",
    "    # Adjust time_bins and sleep_mask to match truncated data\n",
    "    time_bins = time_bins[:all_min_length]\n",
    "    sleep_mask = sleep_mask[:all_min_length]\n",
    "    wake_mask = ~sleep_mask\n",
    "    \n",
    "    # Normalize each cluster's firing rate by its 95th percentile\n",
    "    normalized_counts = np.zeros_like(merged_counts, dtype=float)\n",
    "    for i in range(merged_counts.shape[0]):\n",
    "        cluster_counts = merged_counts[i, :]\n",
    "        p95 = np.percentile(cluster_counts, 95)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if p95 > 0:\n",
    "            normalized_counts[i, :] = cluster_counts / p95\n",
    "        else:\n",
    "            normalized_counts[i, :] = cluster_counts\n",
    "    \n",
    "    # Calculate how many original bins fit into our new larger bins\n",
    "    bins_per_large_bin = int(bin_size_s / original_bin_size)\n",
    "    num_large_bins = len(time_bins) // bins_per_large_bin\n",
    "    \n",
    "    if num_large_bins == 0:\n",
    "        print(f\"Warning: Recording too short for {bin_size_s}s bins. Try a smaller bin size.\")\n",
    "        return {}\n",
    "    \n",
    "    # Initialize arrays to store state and firing rates for each large bin\n",
    "    large_bin_states = []\n",
    "    large_bin_times = []\n",
    "    large_bin_firing_rates = np.zeros((normalized_counts.shape[0], num_large_bins))\n",
    "    \n",
    "    # Process each large bin\n",
    "    for i in range(num_large_bins):\n",
    "        start_idx = i * bins_per_large_bin\n",
    "        end_idx = min((i + 1) * bins_per_large_bin, len(time_bins))\n",
    "        \n",
    "        # Calculate bin center time\n",
    "        bin_center = np.mean(time_bins[start_idx:end_idx])\n",
    "        large_bin_times.append(bin_center)\n",
    "        \n",
    "        # Calculate state for this bin\n",
    "        sleep_fraction = np.sum(sleep_mask[start_idx:end_idx]) / (end_idx - start_idx)\n",
    "        \n",
    "        if sleep_fraction >= state_threshold:\n",
    "            large_bin_states.append('Sleep')\n",
    "        elif sleep_fraction <= (1 - state_threshold):\n",
    "            large_bin_states.append('Wake')\n",
    "        else:\n",
    "            large_bin_states.append('Mixed')\n",
    "        \n",
    "        # Calculate normalized firing rate for each cluster in this bin\n",
    "        for j in range(normalized_counts.shape[0]):\n",
    "            normalized_activity_in_bin = np.mean(normalized_counts[j, start_idx:end_idx])\n",
    "            large_bin_firing_rates[j, i] = normalized_activity_in_bin\n",
    "    \n",
    "    # Convert to arrays for easier manipulation\n",
    "    large_bin_times = np.array(large_bin_times)\n",
    "    large_bin_states = np.array(large_bin_states)\n",
    "    \n",
    "    # === STATE DISTRIBUTION ANALYSIS ===\n",
    "    # Collect data for plotting\n",
    "    plot_data = []\n",
    "    for i, cluster_id in enumerate(all_cluster_ids):\n",
    "        sleep_rates = large_bin_firing_rates[i, [idx for idx, state in enumerate(large_bin_states) if state == 'Sleep']]\n",
    "        wake_rates = large_bin_firing_rates[i, [idx for idx, state in enumerate(large_bin_states) if state == 'Wake']]\n",
    "        \n",
    "        # Calculate average normalized firing rate across bins for each state\n",
    "        if len(sleep_rates) > 0:\n",
    "            sleep_avg_rate = np.mean(sleep_rates)\n",
    "            plot_data.append({\n",
    "                'cluster_id': cluster_id,\n",
    "                'state': 'Sleep',\n",
    "                'firing_rate': sleep_avg_rate\n",
    "            })\n",
    "            \n",
    "        if len(wake_rates) > 0:\n",
    "            wake_avg_rate = np.mean(wake_rates)\n",
    "            plot_data.append({\n",
    "                'cluster_id': cluster_id,\n",
    "                'state': 'Wake',\n",
    "                'firing_rate': wake_avg_rate\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame for seaborn\n",
    "    df_plot = pd.DataFrame(plot_data)\n",
    "    \n",
    "    # === STABILITY ANALYSIS ===\n",
    "    # Identify recording midpoint\n",
    "    midpoint_time = (time_bins[0] + time_bins[-1]) / 2\n",
    "    \n",
    "    # Create bin groups\n",
    "    first_half_sleep_bins = np.where((large_bin_times < midpoint_time) & (large_bin_states == 'Sleep'))[0]\n",
    "    first_half_wake_bins = np.where((large_bin_times < midpoint_time) & (large_bin_states == 'Wake'))[0]\n",
    "    second_half_sleep_bins = np.where((large_bin_times >= midpoint_time) & (large_bin_states == 'Sleep'))[0]\n",
    "    second_half_wake_bins = np.where((large_bin_times >= midpoint_time) & (large_bin_states == 'Wake'))[0]\n",
    "    \n",
    "    # Print bin distribution\n",
    "    print(f\"\\nBin distribution for merged probes:\")\n",
    "    print(f\"First half sleep bins: {len(first_half_sleep_bins)}\")\n",
    "    print(f\"First half wake bins: {len(first_half_wake_bins)}\")\n",
    "    print(f\"Second half sleep bins: {len(second_half_sleep_bins)}\")\n",
    "    print(f\"Second half wake bins: {len(second_half_wake_bins)}\")\n",
    "    \n",
    "    # Try to find a balanced split for stability analysis\n",
    "    all_first_half_bins = np.concatenate([first_half_sleep_bins, first_half_wake_bins])\n",
    "    all_second_half_bins = np.concatenate([second_half_sleep_bins, second_half_wake_bins])\n",
    "    all_sleep_bins = np.concatenate([first_half_sleep_bins, second_half_sleep_bins])\n",
    "    all_wake_bins = np.concatenate([first_half_wake_bins, second_half_wake_bins])\n",
    "    \n",
    "    # Define constraints\n",
    "    success = False\n",
    "    iteration = 0\n",
    "    bin_assignment = np.zeros(num_large_bins, dtype=int)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while not success and iteration < max_iterations:\n",
    "        # Create a random assignment (1 = C1, 0 = C2)\n",
    "        bin_assignment = np.zeros(num_large_bins, dtype=int)\n",
    "        \n",
    "        # Randomly assign bins to C1 (1) or C2 (0)\n",
    "        all_bins = np.arange(num_large_bins)\n",
    "        usable_bins = np.where(np.isin(all_bins, np.concatenate([all_first_half_bins, all_second_half_bins])))[0]\n",
    "        \n",
    "        if len(usable_bins) > 0:\n",
    "            # Randomly select ~50% of bins for C1\n",
    "            c1_bins = np.random.choice(usable_bins, size=len(usable_bins)//2, replace=False)\n",
    "            bin_assignment[c1_bins] = 1\n",
    "            \n",
    "            # Check balance criteria\n",
    "            if len(all_first_half_bins) > 0 and len(all_second_half_bins) > 0:\n",
    "                first_half_in_c1 = np.sum(bin_assignment[all_first_half_bins])\n",
    "                first_half_in_c2 = len(all_first_half_bins) - first_half_in_c1\n",
    "                \n",
    "                second_half_in_c1 = np.sum(bin_assignment[all_second_half_bins])\n",
    "                second_half_in_c2 = len(all_second_half_bins) - second_half_in_c1\n",
    "                \n",
    "                # Check temporal balance (no more than 70% from one half)\n",
    "                if first_half_in_c1 + second_half_in_c1 > 0 and first_half_in_c2 + second_half_in_c2 > 0:\n",
    "                    temporal_balance_c1 = (first_half_in_c1 / (first_half_in_c1 + second_half_in_c1) <= 0.7 and\n",
    "                                          second_half_in_c1 / (first_half_in_c1 + second_half_in_c1) <= 0.7)\n",
    "                    \n",
    "                    temporal_balance_c2 = (first_half_in_c2 / (first_half_in_c2 + second_half_in_c2) <= 0.7 and\n",
    "                                          second_half_in_c2 / (first_half_in_c2 + second_half_in_c2) <= 0.7)\n",
    "                    \n",
    "                    # If both criteria are met, we're successful\n",
    "                    if temporal_balance_c1 and temporal_balance_c2:\n",
    "                        success = True\n",
    "                    elif iteration > max_iterations//2:\n",
    "                        success = True  # Accept less ideal split after many attempts\n",
    "        \n",
    "        iteration += 1\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    if not success:\n",
    "        print(f\"Could not find a balanced split after {max_iterations} iterations. Using best available split.\")\n",
    "    else:\n",
    "        print(f\"Found balanced split after {iteration} iterations ({end_time - start_time:.2f}s)\")\n",
    "    \n",
    "    # Calculate firing rates for each category\n",
    "    c1_mask = bin_assignment == 1\n",
    "    c2_mask = bin_assignment == 0\n",
    "    \n",
    "    # Calculate average normalized firing rates in C1 and C2 for each cluster\n",
    "    c1_rates = np.zeros(normalized_counts.shape[0])\n",
    "    c2_rates = np.zeros(normalized_counts.shape[0])\n",
    "    \n",
    "    # Also calculate sleep-wake difference for each category\n",
    "    c1_sleep_rates = np.zeros(normalized_counts.shape[0])\n",
    "    c1_wake_rates = np.zeros(normalized_counts.shape[0])\n",
    "    c2_sleep_rates = np.zeros(normalized_counts.shape[0])\n",
    "    c2_wake_rates = np.zeros(normalized_counts.shape[0])\n",
    "    \n",
    "    for i in range(normalized_counts.shape[0]):\n",
    "        # Average normalized rates in each category\n",
    "        c1_rates[i] = np.mean(large_bin_firing_rates[i, c1_mask]) if np.sum(c1_mask) > 0 else np.nan\n",
    "        c2_rates[i] = np.mean(large_bin_firing_rates[i, c2_mask]) if np.sum(c2_mask) > 0 else np.nan\n",
    "        \n",
    "        # Sleep/wake rates in C1\n",
    "        c1_sleep_mask = c1_mask & np.isin(np.arange(num_large_bins), all_sleep_bins)\n",
    "        c1_wake_mask = c1_mask & np.isin(np.arange(num_large_bins), all_wake_bins)\n",
    "        \n",
    "        c1_sleep_rates[i] = np.mean(large_bin_firing_rates[i, c1_sleep_mask]) if np.sum(c1_sleep_mask) > 0 else np.nan\n",
    "        c1_wake_rates[i] = np.mean(large_bin_firing_rates[i, c1_wake_mask]) if np.sum(c1_wake_mask) > 0 else np.nan\n",
    "        \n",
    "        # Sleep/wake rates in C2\n",
    "        c2_sleep_mask = c2_mask & np.isin(np.arange(num_large_bins), all_sleep_bins)\n",
    "        c2_wake_mask = c2_mask & np.isin(np.arange(num_large_bins), all_wake_bins)\n",
    "        \n",
    "        c2_sleep_rates[i] = np.mean(large_bin_firing_rates[i, c2_sleep_mask]) if np.sum(c2_sleep_mask) > 0 else np.nan\n",
    "        c2_wake_rates[i] = np.mean(large_bin_firing_rates[i, c2_wake_mask]) if np.sum(c2_wake_mask) > 0 else np.nan\n",
    "    \n",
    "    # Calculate modulation\n",
    "    c1_modulation = c1_wake_rates - c1_sleep_rates\n",
    "    c2_modulation = c2_wake_rates - c2_sleep_rates\n",
    "    \n",
    "    # === COMBINED PLOTTING ===\n",
    "    # Create figure with 3 subplots: swarm plot + 2 stability plots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
    "    \n",
    "    # === PLOT 1: STATE DISTRIBUTION SWARM PLOT ===\n",
    "    if not df_plot.empty:\n",
    "        # Create swarm plot\n",
    "        sns.swarmplot(data=df_plot, x='state', y='firing_rate', color='gray', alpha=0.7, size=5, ax=axes[0])\n",
    "        \n",
    "        # Add box plot over swarm plot\n",
    "        sns.boxplot(data=df_plot, x='state', y='firing_rate', color='white', fliersize=0, width=0.5, \n",
    "                   boxprops={\"facecolor\": (.9, .9, .9, 0.5), \"edgecolor\": \"black\"}, ax=axes[0])\n",
    "        \n",
    "        # Perform statistical test\n",
    "        sleep_rates_plot = df_plot[df_plot['state'] == 'Sleep']['firing_rate'].values\n",
    "        wake_rates_plot = df_plot[df_plot['state'] == 'Wake']['firing_rate'].values\n",
    "        \n",
    "        if len(sleep_rates_plot) > 0 and len(wake_rates_plot) > 0:\n",
    "            stat, p_value = stats.mannwhitneyu(sleep_rates_plot, wake_rates_plot)\n",
    "            axes[0].set_title('State-Dependent Firing Rates', fontsize=14, fontweight='bold')\n",
    "        else:\n",
    "            axes[0].set_title('State-Dependent Firing Rates', fontsize=14, fontweight='bold')\n",
    "    else:\n",
    "        axes[0].text(0.5, 0.5, 'No data available for state analysis', \n",
    "                    ha='center', va='center', transform=axes[0].transAxes)\n",
    "        axes[0].set_title('State-Dependent Firing Rates', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Remove x-axis label and make y-axis label bigger\n",
    "    axes[0].set_xlabel('')\n",
    "    axes[0].set_ylabel('Normalized Firing Rate', fontsize=12)\n",
    "    axes[0].tick_params(axis='x', labelsize=12)\n",
    "    axes[0].grid(True, axis='y', alpha=0.3)\n",
    "    \n",
    "    # === PLOT 2: STABILITY - FIRING RATE CONSISTENCY ===\n",
    "    valid_mask = ~np.isnan(c1_rates) & ~np.isnan(c2_rates)\n",
    "    \n",
    "    if np.sum(valid_mask) > 1:\n",
    "        # Get max value for axis scaling\n",
    "        max_val = max(np.nanmax(c1_rates), np.nanmax(c2_rates))\n",
    "        \n",
    "        # Create scatter plot\n",
    "        axes[1].scatter(c1_rates[valid_mask], c2_rates[valid_mask], alpha=0.7)\n",
    "        \n",
    "        # Add identity line\n",
    "        axes[1].plot([0, max_val*1.1], [0, max_val*1.1], 'k--', alpha=0.7)\n",
    "        \n",
    "        # Add regression line\n",
    "        slope, intercept, r_value, p_value_stability, std_err = stats.linregress(\n",
    "            c1_rates[valid_mask], c2_rates[valid_mask]\n",
    "        )\n",
    "        \n",
    "        x_vals = np.array([0, max_val*1.1])\n",
    "        axes[1].plot(x_vals, intercept + slope * x_vals, 'r-', alpha=0.7,\n",
    "                    label=f'r² = {r_value**2:.2f}, p = {\"< 0.001\" if p_value_stability < 0.001 else f\"{p_value_stability:.3f}\"}')\n",
    "        \n",
    "        axes[1].set_xlabel('C1 - Normalized Firing Rate', fontsize=12)\n",
    "        axes[1].set_ylabel('C2 - Normalized Firing Rate', fontsize=12)\n",
    "        axes[1].set_title('Firing Rate Stability', fontsize=14, fontweight='bold')\n",
    "        axes[1].legend(fontsize=9)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, 'Insufficient data for\\nstability analysis',\n",
    "                   ha='center', va='center', transform=axes[1].transAxes)\n",
    "        axes[1].set_title('Firing Rate Stability', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel('C1 - Normalized Firing Rate', fontsize=12)\n",
    "        axes[1].set_ylabel('C2 - Normalized Firing Rate', fontsize=12)\n",
    "    \n",
    "    # === PLOT 3: STABILITY - MODULATION CONSISTENCY ===\n",
    "    valid_mod_mask = (~np.isnan(c1_modulation) & ~np.isnan(c2_modulation))\n",
    "    \n",
    "    if np.sum(valid_mod_mask) > 1:\n",
    "        # Get max absolute value for axis scaling\n",
    "        max_mod = max(\n",
    "            np.nanmax(np.abs(c1_modulation)), \n",
    "            np.nanmax(np.abs(c2_modulation))\n",
    "        ) * 1.1\n",
    "        \n",
    "        # Create scatter plot\n",
    "        axes[2].scatter(c1_modulation[valid_mod_mask], c2_modulation[valid_mod_mask], alpha=0.7)\n",
    "        \n",
    "        # Add identity line\n",
    "        axes[2].plot([-max_mod, max_mod], [-max_mod, max_mod], 'k--', alpha=0.7)\n",
    "        \n",
    "        # Add regression line\n",
    "        mod_slope, mod_intercept, mod_r, mod_p, mod_err = stats.linregress(\n",
    "            c1_modulation[valid_mod_mask], c2_modulation[valid_mod_mask]\n",
    "        )\n",
    "        \n",
    "        x_mod_vals = np.array([-max_mod, max_mod])\n",
    "        axes[2].plot(x_mod_vals, mod_intercept + mod_slope * x_mod_vals, 'r-', alpha=0.7,\n",
    "                    label=f'r² = {mod_r**2:.2f}, p = {\"< 0.001\" if mod_p < 0.001 else f\"{mod_p:.3f}\"}')\n",
    "        \n",
    "        # Add quadrant lines\n",
    "        axes[2].axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "        axes[2].axvline(x=0, color='gray', linestyle='-', alpha=0.3)\n",
    "        \n",
    "        # Count points in each quadrant\n",
    "        q1 = np.sum((c1_modulation > 0) & (c2_modulation > 0) & valid_mod_mask)\n",
    "        q2 = np.sum((c1_modulation < 0) & (c2_modulation > 0) & valid_mod_mask)\n",
    "        q3 = np.sum((c1_modulation < 0) & (c2_modulation < 0) & valid_mod_mask)\n",
    "        q4 = np.sum((c1_modulation > 0) & (c2_modulation < 0) & valid_mod_mask)\n",
    "        \n",
    "        axes[2].set_xlabel('C1 - Sleep-Wake Modulation', fontsize=12)\n",
    "        axes[2].set_ylabel('C2 - Sleep-Wake Modulation', fontsize=12)\n",
    "        axes[2].set_title('Firing Rate Stability across States', fontsize=14, fontweight='bold')\n",
    "        axes[2].legend(fontsize=9)\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Set equal x and y limits\n",
    "        axes[2].set_xlim(-max_mod, max_mod)\n",
    "        axes[2].set_ylim(-max_mod, max_mod)\n",
    "        \n",
    "    else:\n",
    "        axes[2].text(0.5, 0.5, 'Insufficient data for\\nmodulation analysis',\n",
    "                   ha='center', va='center', transform=axes[2].transAxes)\n",
    "        axes[2].set_title('Firing Rate Stability across States', fontsize=14, fontweight='bold')\n",
    "        axes[2].set_xlabel('C1 - Sleep-Wake Modulation', fontsize=12)\n",
    "        axes[2].set_ylabel('C2 - Sleep-Wake Modulation', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot if requested\n",
    "    if save_plots and output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"combined_state_stability_analysis_{timestamp}.png\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved combined analysis plot to: {filepath}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Store and return combined results\n",
    "    combined_results = {\n",
    "        'merged': {\n",
    "            # State distribution results\n",
    "            'cluster_ids': all_cluster_ids,\n",
    "            'large_bin_states': large_bin_states,\n",
    "            'large_bin_firing_rates': large_bin_firing_rates,\n",
    "            'large_bin_centers': large_bin_times,\n",
    "            'plot_data': df_plot,\n",
    "            \n",
    "            # Stability results\n",
    "            'c1_rates': c1_rates,\n",
    "            'c2_rates': c2_rates,\n",
    "            'c1_sleep_rates': c1_sleep_rates,\n",
    "            'c1_wake_rates': c1_wake_rates,\n",
    "            'c2_sleep_rates': c2_sleep_rates,\n",
    "            'c2_wake_rates': c2_wake_rates,\n",
    "            'c1_modulation': c1_modulation,\n",
    "            'c2_modulation': c2_modulation,\n",
    "            \n",
    "            # Analysis parameters\n",
    "            'bin_size_s': bin_size_s,\n",
    "            'state_threshold': state_threshold,\n",
    "            'num_large_bins': num_large_bins\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c550e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_results = analyze_cluster_state_and_stability(\n",
    "    results=results,\n",
    "    output_dir=output_folder,\n",
    "    save_plots=True,\n",
    "    bin_size_s=120,  # 2 minutes\n",
    "    state_threshold=0.9,  # 90%\n",
    "    max_iterations=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a7bef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PinkRigs_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
